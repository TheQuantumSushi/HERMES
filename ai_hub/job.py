# job.py

"""
Automatically submits jobs to the Qualcomm AI Hub through the API/library.
It first traces the PyTorch model generated by the training script, then
submits the following jobs :
- compiling
- profiling
- running inference
The metrics are then collected and saved.
Under the HERMES/ai_hub folder, each job gets a folder with its name, and
a summary.txt file inside with the infos/metrics.
"""

### IMPORT LIBRARIES :

import os
import time
import logging
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
import qai_hub as hub

### DEFINE PATHS :

HERMES_ROOT = os.environ["HERMES_ROOT"] # load from environment variable
LOG_PATH = os.path.join(HERMES_ROOT, "logs.txt")
SCRIPT_DIR = os.path.join(HERMES_ROOT, "scripts")
ML_DIR = os.path.join(HERMES_ROOT, "ml")
MODEL_PTH = os.path.join(ML_DIR, "models", "cnn_regressor.pth")
TRACED_MODEL  = "cnn_regressor_traced.pt"

### DEFINE PARAMETERS :

INPUT_SHAPE = (1, 3, 128, 128)
DEVICE_FAMILY = "Samsung Galaxy S24 (Family)"
INPUT_IMAGE = os.path.join(HERMES_ROOT, "ml", "predictions", "prediction_2025-05-10", "photo.png")

### SETUP LOGGING :

logging.basicConfig(
    filename = LOG_PATH,
    level = logging.INFO,
    format = "%(asctime)s [job] %(message)s",
    datefmt = "%Y-%m-%d %H:%M:%S",
)

### DEFINE MODEL CLASS :

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.features = nn.Sequential(
            nn.Conv2d(3, 16, 5, stride = 2, padding = 2), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(16, 32, 3, stride = 1, padding = 1), nn.ReLU(), nn.MaxPool2d(2),
            nn.Conv2d(32, 64, 3, stride = 1, padding = 1), nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1)),
        )
        self.regressor = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 4)
        )

    def forward(self, x):
        return self.regressor(self.features(x))

### DEFINE TOOL FUNCTIONS :

def trace_model():
    """
    Tracing the PyTorch model (conversion to TorchScript model).
    """

    logging.info("Starting model trace")
    device = torch.device("cpu")
    model = SimpleCNN().to(device)
    model.load_state_dict(torch.load(MODEL_PTH, map_location = device))
    model.eval()
    dummy = torch.rand(INPUT_SHAPE, device = device)
    traced = torch.jit.trace(model, dummy)
    traced.save(TRACED_MODEL)
    logging.info(f"Model traced and saved to {TRACED_MODEL}")


def submit_compile():
    """
    Submitting a compilation job to the Qualcomm AI Hub.
    Compilation converts the traced model into a hardware-optimized
    version, including quantization or operator fusion.

    Returns :
        - hub.CompileJob : encapsulates all the metadata returned by the compilation job's completion
    """

    logging.info("Submitting compile job")

    # Submit the job :
    compile_job = hub.submit_compile_job(
        model = TRACED_MODEL,
        device = hub.Device(DEVICE_FAMILY),
        input_specs = {"input": INPUT_SHAPE},
        options = "--target_runtime tflite"
    )
    logging.info(f"Compile job scheduled : ID = {compile_job.job_id}, URL = {compile_job.url}")

    # Check output for success/errors :
    status = compile_job.wait() # wait for the completion of the job
    logging.info(f"Compile job completed with status : {status.code}")
    if status != "SUCCESS":
        logging.error(f"Compile job failed : {status.code}")
        raise RuntimeError(f"Compile job failed with status : {status.code}")

    return compile_job

def submit_profile(compile_job):
    """
    Submitting a profiling job to the Qualcomm AI Hub.
    Profiling measures runtime performance ((latency, throughput,
    memory usage, power, etc.)) on target devices, and helps
    development by understanding the behavior of the model.

    Args :
        - compile_job [hub.CompileJob] : the compilation job from which will be extracted the compiled model

    Returns :
        - hub.ProfileJob : encapsulates all the metadata returned by the profiling job's completion
    """

    target_model = compile_job.get_target_model()
    logging.info("Submitting profile job")

    # Submit the job :
    profile_job = hub.submit_profile_job(
        model = target_model,
        device = hub.Device(DEVICE_FAMILY)
    )
    logging.info(f"Profile job scheduled: ID = {profile_job.job_id}, URL = {profile_job.url}")

    # Check output for success/errors :
    status = profile_job.wait() # wait for the completion of the job
    logging.info(f"Profile job completed with status : {status.code}")
    if status != "SUCCESS":
        logging.error(f"Profile job failed : {status.code}")
        raise RuntimeError(f"Profile job failed with status : {status.code}")

    return profile_job

def submit_inference(compile_job):
    """
    Submitting an inference running job to the Qualcomm AI Hub.
    The job then returns the metrics of the inference, and the prediction made.

    Args :
        - compile_job [hub.CompileJob] : the compilation job from which will be extracted the compiled model

    Returns :
        - hub.InferenceJob : encapsulates all the metadata returned by the inference running job's
                             completion, including predictions
    """

    target_model = compile_job.get_target_model()
    logging.info("Preparing inference input image")

    # Submit the job :
    img = Image.open(INPUT_IMAGE).resize((128, 128)).convert("RGB")
    arr = np.transpose(np.array(img)/255.0, (2, 0, 1)).astype(np.float32)[None, :, :, :]
    logging.info("Submitting inference job")
    inf_job = hub.submit_inference_job(
        model = target_model,
        device = hub.Device(DEVICE_FAMILY),
        inputs = {"input": [arr]}
    )
    logging.info(f"Inference job scheduled: ID = {inf_job.job_id}, URL = {inf_job.url}")

    # Check output for success/errors :
    status = inf_job.wait() # wait for the completion of the job
    logging.info(f"Inference job completed with status : {status.code}")
    if status != "SUCCESS":
        logging.error(f"Inference job failed : {status.code}")
        raise RuntimeError(f"Inference job failed with status : {status.code}")

    return inf_job

def download_results(inf_job):
    """
    Extracting the data, metrics and predictions from the completed inference job, and logging it.

    Args :
        - inf_job [hub.InferenceJob] : encapsulates all the metadata returned by the inference
                                       running job's completion, including predictions

    Returns :
        - dict[str, Any] : the output data to parse and save
    """

    outputs = inf_job.download_output_data()
    logging.info(f"Downloaded inference outputs: {outputs}")

    return outputs


def save_summary(compile_job, profile_job, inf_job, outputs):
    """
    Generating and saving a summary file under HERMES/ai_hub/job_XXXXXXXXX/summary.txt,
    which includes infos about the compiling, profiling, inference running jobs, and
    the raw output, as well as the prediction (parsed from the raw output).

    Args :
        - compile_job [hub.InferenceJob] : the compiling job metadata
        - profile_job [hub.ProfileJob] : the profiling job metadata
        - inf_job [hub.CompileJob] : the inference job metadata
        - outputs [dict[str, Any]] : the output data from the inference job
    """

    # Create the folder for the job summary (which is arbitrarily named after the compiling job) :
    job_dir = os.path.join(os.path.dirname(__file__), "job_" + compile_job.job_id)
    os.makedirs(job_dir, exist_ok = True)
    summary_path = os.path.join(job_dir, "summary.txt")

    # Parse the raw outputs :
    output_array = outputs['output_0'][0]
    parsed_output = output_array.tolist().tolist()
    logging.info(f"Parsed output : {parsed_output}")

    # Write the summary into the file :
    with open(summary_path, "w") as f:
        f.write(".\n")
        f.write("└── Job summary\n")
        f.write("    ├── Compilation :\n")
        f.write(f"    │   ├── Compile Job ID : {compile_job.job_id}\n")
        f.write(f"    │   └── Compile URL : {compile_job.url}\n")
        f.write("    ├── Profiling :\n")
        f.write(f"    │   ├── Profile Job ID : {profile_job.job_id}\n")
        f.write(f"    │   └── Profile URL : {profile_job.url}\n")
        f.write("    ├── Inference :\n")
        f.write(f"    │   ├── Inference Job ID : {inf_job.job_id}\n")
        f.write(f"    │   └── Inference URL : {inf_job.url}\n")
        f.write("    └── Output :\n")
        f.write(f"        ├── Raw output : {outputs}\n")
        f.write(f"        └── Parsed prediction : {parsed_output}\n")
    logging.info(f"Summary saved to {summary_path}")

### EXECUTION OF THE JOBS IN ORDER :

def main():
    """
    Submit all jobs to the AI Hub and gather the output to save it.
    """

    # Initialize logging :
    start = time.time()
    logging.info("Job script started")

    # Trace the model :
    trace_model()

    # Submit jobs :
    compile_job = submit_compile()
    profile_job = submit_profile(compile_job)
    inf_job = submit_inference(compile_job)

    # Get the results and save them in the summary.txt file :
    outputs = download_results(inf_job)
    save_summary(compile_job, profile_job, inf_job, outputs)

    # Log execution time of this program :
    duration = time.time() - start
    logging.info(f"Job script completed in {duration:.2f} seconds")

# Example usage :
if __name__ == "__main__":
    main()
